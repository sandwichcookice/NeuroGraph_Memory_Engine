# 可微分神經記憶網路＋STM/LTM 混合架構

以下文檔描述了一套端到端可微分的記憶系統，用於處理「狀態→動作→狀態」序列並支持短期與長期記憶。該架構採用動態圖結構和圖神經網路（GNN），並結合類似 LLM 的上下文融合機制。

---

## 一、核心模組說明

### 1. 文字編碼器 (Encoder)

* **職責**：將任意純文字形式的環境狀態描述映射至一個固定維度的向量形式，統一後續處理介面。
* **流程**：詞嵌入 → MLP 或 Transformer 層 → 輸出狀態向量 `x_t ∈ ℝ^d`。
* **實踐建議**：可使用 HuggingFace Transformers 的小型模型，或自行用 PyTorch 定義詞嵌入及簡易 MLP。

### 2. 短期記憶 (STM)

* **職責**：持續記錄並強化最近的「狀態→動作→狀態」事件，保留有效的時序記憶片段。
* **資料結構**：

  * 節點 `v_i`：對應每個時間步的狀態 embedding `h_i = x_i`。
  * 有向邊 `(v_t → v_{t+1})`：標記動作 `a_t`，並維護權重 `w^{STM}`。
* **更新機制**：

  1. **新增節點**：每執行動作後，將新狀態作為節點加入圖中。
  2. **新增/強化邊**：若邊不存在，`w^{STM}=w_0`；若已存在，`w^{STM} += Δw(r_t)`。
  3. **快速衰減 & Prune**：`w^{STM} *= γ_s`，並刪除 `w^{STM} < θ_s` 的邊與孤立節點。
* **實踐建議**：可利用 PyTorch Geometric 的動態圖結構實現，或自行管理 NetworkX 圖與權重 tensor。

### 3. 長期記憶 (LTM)

* **職責**：跨訓練回合穩定保存經 Consolidation 加強的重要狀態轉移，形成持久記憶。
* **資料結構**：

  * 節點同 STM，表示歷史狀態。
  * 有向邊 `(v_i → v_j)`：維護 `w^{LTM}`，可隨批次更新與緩慢衰減。
* **Consolidation 更新**：

  1. 遍歷 STM 中所有邊 `(i → j)`，在 LTM 中新增或增強：`w^{LTM} += β·w^{STM}`。
  2. 緩慢衰減 & Prune：`w^{LTM} *= γ_L`，並刪除 `w^{LTM} < θ_L`。
* **實踐建議**：採用 GNN 框架（如 PyG）管理節點與可訓練邊權。

### 4. 記憶讀取器 (ReadNet)

* **職責**：根據當前狀態向量 `x_t`，同時從 STM 與 LTM 中檢索並聚合相關記憶訊息，輸出融合向量 `r_t`。
* **處理流程**：

  1. **STM Attention**：計算 `α_{ij}`，聚合 STM 中關鍵訊息 → `r_t^S`。
  2. **LTM GNN**：在 LTM 圖上進行多層 message-passing → `r_t^L`。
  3. **融合**：拼接 `[x_t; r_t^S; r_t^L]`，經 MLP 輸出最終 `r_t`。
* **實踐建議**：自定義 attention 層。

### 5. 動作解碼器 (Decoder)

* **職責**：基於融合後的特徵向量 `r_t` 與 `x_t`，預測動作分布並選擇下一步動作 `a_t`。
* **流程**：MLP → 輸出 `y_t`（logits）→ `a_t = argmax(y_t)` 或 ε-greedy。
* **實踐建議**：使用 PyTorch `nn.Sequential` 定義多層線性層與 activation，並在訓練時加入探索策略。

### 6. 決策接口 (Decision Interface)

* **職責**：給定當前狀態與目標狀態，利用**整個 STM 緩衝區**與 LTM 圖進行上下文式路徑搜尋，返回最佳動作。
* **流程**：

  1. **編碼**：`x_curr = Enc(s_t)`，`x_goal = Enc(s_goal)`。
  2. **融合上下文**：結合 STM 緩衝中的節點/邊與 LTM 圖結構，計算混合邊權：
     `w_comb = α·w^{STM} + (1-α)·w^{LTM}`。
  3. **序列化上下文**：將 STM 中所有 `x_i, w^{STM}` 作為序列上下文，並結合 LTM 圖結構信息形成決策圖。
  4. **圖搜索**：在決策圖上使用 Dijkstra、A\* 或 widest-path 搜尋，從 `x_curr` 到 `x_goal`。
  5. **動作選取**：提取搜尋路徑的首步動作 `a_t`。
* **實踐建議**：可利用 NetworkX 的圖演算法，或自行實作 A\*，並映射回動作命令。

### 7. 睡眠回放 (Consolidator)

* **職責**：定期（例如手動觸發或條件滿足時）將 STM 中累積的記憶批次更新至 LTM，維護長期記憶品質。
* **流程**：

  1. 遍歷 STM 邊 `(i → j)`。
  2. 在 LTM 中新增或增強：`w^{LTM}_{ij} += β·w^{STM}_{ij}`。
  3. 分別對 STM、LTM 執行衰減 (`γ_s, γ_L`) 與 Prune。
* **實踐建議**：設計定時器或檢查回合數，自動觸發 Consolidation 函數。

---

## 二、實踐步驟總覽

1. **實作 Encoder**：詞嵌入 + MLP/Transformer。
2. **實作 STM 結構**：動態圖更新、邊權增強與快速衰減。
3. **實作 LTM 結構**：持久化圖、Consolidation 更新與慢速衰減。
4. **實作 ReadNet**：STM 注意力 + LTM GNN + 融合 MLP。
5. **實作 Decoder**：MLP 輸出動作 logits + 探索策略。
6. **實作 Decision Interface**：上下文序列化 + 圖搜尋 + 動作提取。
7. **實作 Consolidator**：批次 STM→LTM 更新 + 剪枝機制。
8. **參數調整**：`w_0, Δw, γ_s, θ_s, β, γ_L, θ_L, α`。

